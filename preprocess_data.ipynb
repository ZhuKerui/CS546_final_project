{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocess_data import (raw_train_data_file, raw_dev_data_file, raw_test_data_file, processed_dev_data_file, processed_test_data_file, processed_train_data_file, generate_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data = json.load(open(raw_train_data_file))['entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = [data_sample[list(data_sample.keys())[0]] for data_sample in raw_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12876/12876 [00:00<00:00, 53692.30it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data_samples = []\n",
    "for data_sample in tqdm.tqdm(data_samples):\n",
    "    new_data_sample = data_sample.copy()\n",
    "    unused_labels = ['originaltriplesets', 'xml_id', 'size', 'shape', 'shape_type']\n",
    "    for label in unused_labels:\n",
    "        new_data_sample.pop(label)\n",
    "    triples = new_data_sample['modifiedtripleset']\n",
    "    new_data_sample['input_seq'], new_data_sample['properties'] = generate_seq(triples)\n",
    "    new_data_sample['target_sents'] = [sent['lex'] for sent in new_data_sample.pop('lexicalisations')]\n",
    "    new_data_samples.append(new_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Airport',\n",
       " 'modifiedtripleset': [{'object': '\"Aarhus, Denmark\"',\n",
       "   'property': 'cityServed',\n",
       "   'subject': 'Aarhus_Airport'}],\n",
       " 'input_seq': ['Aarhus_Airport', '[P]', '\"Aarhus, Denmark\"'],\n",
       " 'properties': ['cityServed'],\n",
       " 'target_sents': ['The Aarhus is the airport of Aarhus, Denmark.',\n",
       "  'Aarhus Airport serves the city of Aarhus, Denmark.']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(new_data_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_train_data_file) as f_in:\n",
    "    train_data = [json.loads(line) for line in f_in]\n",
    "with open(processed_dev_data_file) as f_in:\n",
    "    dev_data = [json.loads(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities_set = set()\n",
    "train_relation_set = set()\n",
    "for data in train_data:\n",
    "    train_entities_set.update([tri['subject'] for tri in data['modifiedtripleset']])\n",
    "    train_entities_set.update([tri['object'] for tri in data['modifiedtripleset']])\n",
    "    train_relation_set.update([tri['property'] for tri in data['modifiedtripleset']])\n",
    "\n",
    "dev_entities_set = set()\n",
    "dev_relation_set = set()\n",
    "for data in dev_data:\n",
    "    dev_entities_set.update([tri['subject'] for tri in data['modifiedtripleset']])\n",
    "    dev_entities_set.update([tri['object'] for tri in data['modifiedtripleset']])\n",
    "    dev_relation_set.update([tri['property'] for tri in data['modifiedtripleset']])\n",
    "    \n",
    "print(len(train_entities_set))\n",
    "print(len(dev_entities_set))\n",
    "uncovered_entities_set = dev_entities_set - train_entities_set\n",
    "print(len(uncovered_entities_set))\n",
    "print('')\n",
    "print(len(train_relation_set))\n",
    "print(len(dev_relation_set))\n",
    "uncovered_relation_set = dev_relation_set - train_relation_set\n",
    "print(len(uncovered_relation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered_entities_set"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef5ebbdb7177042242d5c612e2b6c40616b7b35a1a7eb91b6720a1b19cc3ee4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('autoprompt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
