{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoConfig, AutoModelWithLMHead, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', type=Path, required=True, help='Train data path')\n",
    "parser.add_argument('--dev', type=Path, required=True, help='Dev data path')\n",
    "parser.add_argument('--property-map', type=str, default=None, help='JSON object defining property map')\n",
    "parser.add_argument('--property-prompt', type=str, default=None, help='Tokens for each property')\n",
    "\n",
    "# LAMA-specific\n",
    "parser.add_argument('--tokenize-labels', action='store_true',\n",
    "                    help='If specified labels are split into word pieces.'\n",
    "                            'Needed for LAMA probe experiments.')\n",
    "parser.add_argument('--filter', action='store_true',\n",
    "                    help='If specified, filter out special tokens and gold objects.'\n",
    "                            'Furthermore, tokens starting with capital '\n",
    "                            'letters will not appear in triggers. Lazy '\n",
    "                            'approach for removing proper nouns.')\n",
    "parser.add_argument('--print-lama', action='store_true',\n",
    "                    help='Prints best trigger in LAMA format.')\n",
    "\n",
    "parser.add_argument('--initial-trigger', nargs='+', type=str, default=None, help='Manual prompt')\n",
    "\n",
    "parser.add_argument('--bsz', type=int, default=32, help='Batch size')\n",
    "parser.add_argument('--eval-size', type=int, default=256, help='Eval size')\n",
    "parser.add_argument('--iters', type=int, default=100,\n",
    "                    help='Number of iterations to run trigger search algorithm')\n",
    "parser.add_argument('--accumulation-steps', type=int, default=10)\n",
    "parser.add_argument('--model-name', type=str, default='bert-base-cased',\n",
    "                    help='Model name passed to HuggingFace AutoX classes.')\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--limit', type=int, default=None)\n",
    "parser.add_argument('--use-ctx', action='store_true',\n",
    "                    help='Use context sentences for relation extraction only')\n",
    "parser.add_argument('--perturbed', action='store_true',\n",
    "                    help='Perturbed sentence evaluation of relation extraction: replace each object in dataset with a random other object')\n",
    "parser.add_argument('--patience', type=int, default=5)\n",
    "parser.add_argument('--num-cand', type=int, default=10)\n",
    "parser.add_argument('--sentence-size', type=int, default=50)\n",
    "\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "args = parser.parse_args(['--train', 'webnlg_dataset/webnlg_train.jsonl', '--dev', 'webnlg_dataset/webnlg_dev.jsonl', '--num-cand', '10', '--accumulation-steps', '1', '--model-name', 'facebook/bart-base', '--bsz', '56', '--eval-size', '56', '--iters', '2', '--tokenize-labels', '--filter', '--print-lama'])\n",
    "\n",
    "if args.debug:\n",
    "    level = logging.DEBUG\n",
    "else:\n",
    "    level = logging.INFO\n",
    "logging.basicConfig(level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the relevant random seeds.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "set_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task_specific_tokens(tokenizer):\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': ['[T]', '[Y]']\n",
    "    })\n",
    "    tokenizer.trigger_token = '[T]'\n",
    "    tokenizer.trigger_token_id = tokenizer.convert_tokens_to_ids('[T]')\n",
    "    # NOTE: BERT and RoBERTa tokenizers work properly if [X] is not a special token...\n",
    "    # tokenizer.lama_x = '[X]'\n",
    "    # tokenizer.lama_x_id = tokenizer.convert_tokens_to_ids('[X]')\n",
    "    tokenizer.lama_y = '[Y]'\n",
    "    tokenizer.lama_x_id = tokenizer.convert_tokens_to_ids('[Y]')\n",
    "\n",
    "def load_pretrained(model_name):\n",
    "    \"\"\"\n",
    "    Loads pretrained HuggingFace config/model/tokenizer, as well as performs required\n",
    "    initialization steps to facilitate working with triggers.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    add_task_specific_tokens(tokenizer)\n",
    "    return config, model, tokenizer\n",
    "\n",
    "logger.info('Loading model, tokenizer, etc.')\n",
    "config, model, tokenizer = load_pretrained(args.model_name)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, config):\n",
    "    \"\"\"Returns the wordpiece embedding module.\"\"\"\n",
    "    base_model = model.get_encoder()\n",
    "    embeddings = base_model.embed_tokens\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientStorage:\n",
    "    \"\"\"\n",
    "    This object stores the intermediate gradients of the output a the given PyTorch module, which\n",
    "    otherwise might not be retained.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        self._stored_gradient = None\n",
    "        module.register_backward_hook(self.hook)\n",
    "\n",
    "    def hook(self, module, grad_in, grad_out):\n",
    "        self._stored_gradient = grad_out[0]\n",
    "\n",
    "    def get(self):\n",
    "        return self._stored_gradient\n",
    "    \n",
    "embedding_gradient = GradientStorage(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def default_prompt():\n",
    "    return 0\n",
    "\n",
    "def generate_property_map(fname):\n",
    "    property_map = defaultdict(default_prompt)\n",
    "    token_id = 1\n",
    "    for x in load_jsonl(fname):\n",
    "        for property in x['properties']:\n",
    "            if property not in property_map:\n",
    "                property_map[property] = token_id\n",
    "                token_id += 1\n",
    "    return property_map\n",
    "\n",
    "if args.property_map is not None and args.property_prompt is not None:\n",
    "    property_map = json.loads(args.property_map)\n",
    "    property_prompt = torch.load(args.property_prompt).to(device)\n",
    "    logger.info(f\"Property map: {property_map}\")\n",
    "else:\n",
    "    property_map = generate_property_map(args.train)\n",
    "    property_prompt = torch.ones((len(property_map)+1, 3), dtype=torch.int64) * tokenizer.mask_token_id\n",
    "    property_prompt = property_prompt.to(device)\n",
    "    with open('property_map.json', 'w') as f_out:\n",
    "        json.dump(property_map, f_out)\n",
    "    logger.info('New property map generated')\n",
    "best_property_prompt = property_prompt.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(tokenizer, label):\n",
    "    \"\"\"\n",
    "    Helper function for encoding labels. Deals with the subtleties of handling multiple tokens.\n",
    "    \"\"\"\n",
    "    if isinstance(label, str):\n",
    "        encoded = tokenizer(label, return_tensors='pt')['input_ids']\n",
    "    elif isinstance(label, list):\n",
    "        encoded = torch.tensor(tokenizer.convert_tokens_to_ids(label)).unsqueeze(0)\n",
    "    elif isinstance(label, int):\n",
    "        encoded = torch.tensor([[label]])\n",
    "    return encoded\n",
    "\n",
    "class TriggerTemplatizer:\n",
    "    \"\"\"\n",
    "    An object to facilitate creating transformers-friendly triggers inputs from a template.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    template : str\n",
    "        The template string, comprised of the following tokens:\n",
    "            [T] to mark a trigger placeholder.\n",
    "            [P] to mark a prediction placeholder.\n",
    "            {fields} arbitrary fields instantiated from the dataset instances.\n",
    "        For example a NLI template might look like:\n",
    "            \"[T] [T] [T] {premise} [P] {hypothesis}\"\n",
    "    tokenizer : PretrainedTokenizer\n",
    "        A HuggingFace tokenizer. Must have special trigger and predict tokens.\n",
    "    add_special_tokens : bool\n",
    "        Whether or not to add special tokens when encoding. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 prompt_len:int,\n",
    "                 property_map:dict):\n",
    "        if not hasattr(tokenizer, 'trigger_token'):\n",
    "            raise ValueError(\n",
    "                'Tokenizer missing special trigger and predict tokens in vocab.'\n",
    "                'Use `utils.add_special_tokens` to add them.'\n",
    "            )\n",
    "        self._tokenizer = tokenizer\n",
    "        self._prompt_len = prompt_len\n",
    "        self._property_map = property_map\n",
    "\n",
    "    def __call__(self, format_kwargs:dict):\n",
    "        # Format the template string\n",
    "        format_kwargs = format_kwargs.copy()\n",
    "        label = random.choice(format_kwargs.pop('target_sents'))\n",
    "        input_seq = format_kwargs.pop('input_seq')\n",
    "        properties = format_kwargs.pop('properties')\n",
    "        \n",
    "        if input_seq.count('[P]') != len(properties) or label is None:\n",
    "            raise Exception('Bad data')\n",
    "        \n",
    "        text = ' '.join([word.replace('_', ' ') if word != '[P]' else ''.join(['[T]'] * self._prompt_len) for word in input_seq])\n",
    "\n",
    "        # Have the tokenizer encode the text and process the output to:\n",
    "        # - Create a trigger and predict mask\n",
    "        # - Replace the predict token with a mask token\n",
    "        model_inputs = self._tokenizer(text, return_tensors='pt')\n",
    "        input_ids = model_inputs['input_ids']\n",
    "        trigger_mask = input_ids.eq(self._tokenizer.trigger_token_id)\n",
    "\n",
    "        model_inputs['trigger_mask'] = trigger_mask\n",
    "        model_inputs['properties'] = torch.LongTensor([[self._property_map[p] for p in properties]])\n",
    "\n",
    "        # Encode the label(s)\n",
    "        label_id = encode_label(\n",
    "            tokenizer=self._tokenizer,\n",
    "            label=label\n",
    "        )\n",
    "        \n",
    "        model_inputs['labels'] = label_id\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "templatizer = TriggerTemplatizer(\n",
    "    tokenizer,\n",
    "    prompt_len=3,\n",
    "    property_map=property_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_squeeze_sequence(sequence, *args, **kwargs):\n",
    "    \"\"\"Squeezes fake batch dimension added by tokenizer before padding sequence.\"\"\"\n",
    "    return pad_sequence([x.squeeze(0) for x in sequence], *args, **kwargs)\n",
    "\n",
    "class Collator:\n",
    "    \"\"\"\n",
    "    Collates transformer outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id=0):\n",
    "        self._pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Separate the list of inputs and labels\n",
    "        # Assume that all inputs have the same keys as the first\n",
    "        proto_input = features[0]\n",
    "        keys = list(proto_input.keys())\n",
    "        padded_inputs = {}\n",
    "        for key in keys:\n",
    "            if key == 'input_ids':\n",
    "                padding_value = self._pad_token_id\n",
    "            else:\n",
    "                padding_value = 0\n",
    "            # NOTE: We need to squeeze to get rid of fake batch dim.\n",
    "            sequence = [x[key] for x in features]\n",
    "            padded = pad_squeeze_sequence(sequence, batch_first=True, padding_value=padding_value)\n",
    "            padded_inputs[key] = padded\n",
    "        return padded_inputs\n",
    "\n",
    "logger.info('Loading datasets')\n",
    "collator = Collator(pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trigger_dataset(fname, templatizer):\n",
    "    return [templatizer(x) for x in load_jsonl(fname)]\n",
    "\n",
    "# if args.perturbed:\n",
    "#     train_dataset = load_augmented_trigger_dataset(args.train, templatizer, limit=args.limit)\n",
    "# else:\n",
    "train_dataset = load_trigger_dataset(args.train, templatizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.bsz, shuffle=True, collate_fn=collator)\n",
    "\n",
    "# if args.perturbed:\n",
    "#     dev_dataset = utils.load_augmented_trigger_dataset(args.dev, templatizer)\n",
    "# else:\n",
    "dev_dataset = load_trigger_dataset(args.dev, templatizer)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=args.eval_size, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trigger_tokens(model_inputs:dict, trigger_ids:torch.LongTensor, trigger_mask:torch.BoolTensor):\n",
    "    \"\"\"Replaces the trigger tokens in input_ids.\"\"\"\n",
    "    out = model_inputs.copy()\n",
    "    input_ids = model_inputs['input_ids']\n",
    "    # try:\n",
    "    filled = input_ids.masked_scatter(trigger_mask, trigger_ids)\n",
    "    # except RuntimeError:\n",
    "    #     filled = input_ids\n",
    "    out['input_ids'] = filled\n",
    "    return out\n",
    "\n",
    "class PredictWrapper:\n",
    "    \"\"\"\n",
    "    PyTorch transformers model wrapper. Handles necc. preprocessing of inputs for triggers\n",
    "    experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, model_inputs:dict, property_prompt:torch.LongTensor):\n",
    "        # Copy dict so pop operations don't have unwanted side-effects\n",
    "        model_inputs = model_inputs.copy()\n",
    "        trigger_mask = model_inputs.pop('trigger_mask')\n",
    "        properties = model_inputs.pop('properties')\n",
    "        # predict_mask = model_inputs.pop('predict_mask')\n",
    "        model_inputs = replace_trigger_tokens(model_inputs, property_prompt[properties].squeeze(1), trigger_mask)\n",
    "        if 'labels' in model_inputs:\n",
    "            outputs = self._model(**model_inputs)\n",
    "            return outputs.loss, outputs.logits\n",
    "        else:\n",
    "            outputs = self._model(**model_inputs)\n",
    "            return outputs.logits\n",
    "        # predict_logits = logits.masked_select(predict_mask.unsqueeze(-1)).view(logits.size(0), -1)\n",
    "        \n",
    "    \n",
    "predictor = PredictWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Evaluating')\n",
    "for model_inputs in dev_loader:\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        dev_metric, predict_logits = predictor(model_inputs, property_prompt)\n",
    "logger.info(f'Dev metric: {dev_metric.item()}')\n",
    "\n",
    "best_dev_metric = -float('inf')\n",
    "# Measure elapsed time of trigger search\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotflip_attack(averaged_grad,\n",
    "                   embedding_matrix,\n",
    "                   num_candidates=1):\n",
    "    \"\"\"Returns the top candidate replacements.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        gradient_dot_embedding_matrix = torch.matmul(\n",
    "            averaged_grad,\n",
    "            embedding_matrix.T\n",
    "        )\n",
    "        _, top_k_ids = gradient_dot_embedding_matrix.topk(num_candidates)\n",
    "\n",
    "    return top_k_ids, (gradient_dot_embedding_matrix != 0).any(dim=1)\n",
    "\n",
    "for i in tqdm(range(args.iters)):\n",
    "\n",
    "    logger.info(f'Iteration: {i}')\n",
    "\n",
    "    logger.info('Accumulating Gradient')\n",
    "    model.zero_grad()\n",
    "\n",
    "    train_iter = iter(train_loader)\n",
    "    averaged_grad = None\n",
    "\n",
    "    # Accumulate\n",
    "    for step in range(args.accumulation_steps):\n",
    "\n",
    "        # Shuttle inputs to GPU\n",
    "        try:\n",
    "            model_inputs = next(train_iter)\n",
    "        except:\n",
    "            logger.warning(\n",
    "                'Insufficient data for number of accumulation steps. '\n",
    "                'Effective batch size will be smaller than specified.'\n",
    "            )\n",
    "            break\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        loss, predict_logits = predictor(model_inputs, property_prompt)\n",
    "        loss.backward()\n",
    "\n",
    "        grad = embedding_gradient.get()\n",
    "        bsz, _, emb_dim = grad.size()\n",
    "        selection_mask = model_inputs['trigger_mask'].unsqueeze(-1)\n",
    "        grad = torch.masked_select(grad, selection_mask)\n",
    "        grad = grad.view(-1, 3, emb_dim)\n",
    "        \n",
    "        properties = model_inputs['properties']\n",
    "        properties = properties[properties != 0]\n",
    "        properties_matrix = F.one_hot(properties, num_classes=property_prompt.size(0)) * 1.0\n",
    "        grad_ = torch.swapaxes(torch.swapaxes(grad, 0, 1), 1, 2)\n",
    "        grad_sum_ = torch.matmul(grad_, properties_matrix)\n",
    "\n",
    "        if averaged_grad is None:\n",
    "            averaged_grad = grad_sum_ / args.accumulation_steps\n",
    "        else:\n",
    "            averaged_grad += grad_sum_ / args.accumulation_steps\n",
    "\n",
    "    logger.info('Evaluating Candidates')\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    token_to_flip = torch.randint(0, 3, (property_prompt.size(0),)).to(device)\n",
    "    token_to_flip_one_hot = (F.one_hot(token_to_flip, num_classes=3) * 1.0)\n",
    "    selected_grad = torch.matmul(torch.swapaxes(grad_sum_, 0, 2), token_to_flip_one_hot.unsqueeze(-1)).squeeze(-1)\n",
    "    candidates, valid_properties = hotflip_attack(selected_grad, embeddings.weight, num_candidates=args.num_cand)\n",
    "\n",
    "    current_score = 0\n",
    "    candidate_scores = torch.zeros(args.num_cand, device=device)\n",
    "    # candidate_scores = torch.zeros(args.num_cand)\n",
    "    denom = 0\n",
    "    for step in range(args.accumulation_steps):\n",
    "\n",
    "        try:\n",
    "            model_inputs = next(train_iter)\n",
    "        except:\n",
    "            logger.warning(\n",
    "                'Insufficient data for number of accumulation steps. '\n",
    "                'Effective batch size will be smaller than specified.'\n",
    "            )\n",
    "            break\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        labels = model_inputs['labels']\n",
    "        with torch.no_grad():\n",
    "            eval_metric, predict_logits = predictor(model_inputs, property_prompt)\n",
    "\n",
    "        # Update current score\n",
    "        current_score += eval_metric.sum()\n",
    "        denom += labels.size(0)\n",
    "\n",
    "        # NOTE: Instead of iterating over tokens to flip we randomly change just one each\n",
    "        # time so the gradients don't get stale.\n",
    "        for i in range(candidates.size(1)):\n",
    "\n",
    "            temp_property_prompt = property_prompt.clone()\n",
    "            temp_property_prompt[valid_properties, token_to_flip[valid_properties]] = candidates[valid_properties, i]\n",
    "            with torch.no_grad():\n",
    "                eval_metric, predict_logits = predictor(model_inputs, temp_property_prompt)\n",
    "\n",
    "            candidate_scores[i] += eval_metric.sum()\n",
    "\n",
    "    # TODO: Something cleaner. LAMA templates can't have mask tokens, so if\n",
    "    # there are still mask tokens in the trigger then set the current score\n",
    "    # to -inf.\n",
    "    \n",
    "    # if args.print_lama:\n",
    "    #     if trigger_ids.eq(tokenizer.mask_token_id).any():\n",
    "    #         current_score = float('-inf')\n",
    "\n",
    "    if (candidate_scores > current_score).any():\n",
    "        logger.info('Better trigger detected.')\n",
    "        best_candidate_score = candidate_scores.max()\n",
    "        best_candidate_idx = candidate_scores.argmax()\n",
    "        property_prompt[valid_properties, token_to_flip[valid_properties]] = candidates[valid_properties, best_candidate_idx]\n",
    "        logger.info(f'Train metric: {best_candidate_score / (denom + 1e-13): 0.4f}')\n",
    "    else:\n",
    "        logger.info('No improvement detected. Skipping evaluation.')\n",
    "        continue\n",
    "\n",
    "    logger.info('Evaluating')\n",
    "    # numerator = 0\n",
    "    # denominator = 0\n",
    "    for model_inputs in dev_loader:\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        labels = model_inputs['labels']\n",
    "        with torch.no_grad():\n",
    "            dev_metric, predict_logits = predictor(model_inputs, property_prompt)\n",
    "        # numerator += loss.sum().item()\n",
    "        # denominator += labels.size(0)\n",
    "    # dev_metric = numerator / (denominator + 1e-13)\n",
    "\n",
    "    # logger.info(f'Trigger tokens: {tokenizer.convert_ids_to_tokens(trigger_ids.squeeze(0))}')\n",
    "    logger.info(f'Dev metric: {dev_metric.item()}')\n",
    "\n",
    "    # TODO: Something cleaner. LAMA templates can't have mask tokens, so if\n",
    "    # there are still mask tokens in the trigger then set the current score\n",
    "    # to -inf.\n",
    "    \n",
    "    # if args.print_lama:\n",
    "    #     if best_trigger_ids.eq(tokenizer.mask_token_id).any():\n",
    "    #         best_dev_metric = float('-inf')\n",
    "\n",
    "    if dev_metric > best_dev_metric:\n",
    "        logger.info('Best performance so far')\n",
    "        best_property_prompt = property_prompt.clone()\n",
    "        best_dev_metric = dev_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Best dev metric: {best_dev_metric}')\n",
    "torch.save(best_property_prompt, 'best_property_prompt.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef5ebbdb7177042242d5c612e2b6c40616b7b35a1a7eb91b6720a1b19cc3ee4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('autoprompt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
