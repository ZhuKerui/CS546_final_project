{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelWithLMHead, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', type=Path, required=True, help='Train data path')\n",
    "parser.add_argument('--dev', type=Path, required=True, help='Dev data path')\n",
    "parser.add_argument('--template', type=str, help='Template string')\n",
    "parser.add_argument('--label-map', type=str, default=None, help='JSON object defining label map')\n",
    "\n",
    "# LAMA-specific\n",
    "parser.add_argument('--tokenize-labels', action='store_true',\n",
    "                    help='If specified labels are split into word pieces.'\n",
    "                            'Needed for LAMA probe experiments.')\n",
    "parser.add_argument('--filter', action='store_true',\n",
    "                    help='If specified, filter out special tokens and gold objects.'\n",
    "                            'Furthermore, tokens starting with capital '\n",
    "                            'letters will not appear in triggers. Lazy '\n",
    "                            'approach for removing proper nouns.')\n",
    "parser.add_argument('--print-lama', action='store_true',\n",
    "                    help='Prints best trigger in LAMA format.')\n",
    "\n",
    "parser.add_argument('--initial-trigger', nargs='+', type=str, default=None, help='Manual prompt')\n",
    "parser.add_argument('--label-field', type=str, default='label',\n",
    "                    help='Name of the label field')\n",
    "\n",
    "parser.add_argument('--bsz', type=int, default=32, help='Batch size')\n",
    "parser.add_argument('--eval-size', type=int, default=256, help='Eval size')\n",
    "parser.add_argument('--iters', type=int, default=100,\n",
    "                    help='Number of iterations to run trigger search algorithm')\n",
    "parser.add_argument('--accumulation-steps', type=int, default=10)\n",
    "parser.add_argument('--model-name', type=str, default='bert-base-cased',\n",
    "                    help='Model name passed to HuggingFace AutoX classes.')\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--limit', type=int, default=None)\n",
    "parser.add_argument('--use-ctx', action='store_true',\n",
    "                    help='Use context sentences for relation extraction only')\n",
    "parser.add_argument('--perturbed', action='store_true',\n",
    "                    help='Perturbed sentence evaluation of relation extraction: replace each object in dataset with a random other object')\n",
    "parser.add_argument('--patience', type=int, default=5)\n",
    "parser.add_argument('--num-cand', type=int, default=10)\n",
    "parser.add_argument('--sentence-size', type=int, default=50)\n",
    "\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "args = parser.parse_args(['--train', 'fact-retrieval/trex/P17/train.jsonl', '--dev', 'fact-retrieval/trex/P17/dev.jsonl', '--template', '<s> {sub_label} [T] [T] [T] [P] . </s>', '--num-cand', '10', '--accumulation-steps', '1', '--model-name', 'roberta-large', '--bsz', '56', '--eval-size', '56', '--iters', '1000', '--label-field', 'obj_label', '--tokenize-labels', '--filter', '--print-lama'])\n",
    "\n",
    "if args.debug:\n",
    "    level = logging.DEBUG\n",
    "else:\n",
    "    level = logging.INFO\n",
    "logging.basicConfig(level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the relevant random seeds.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "set_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_task_specific_tokens(tokenizer):\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': ['[T]', '[P]', '[Y]']\n",
    "    })\n",
    "    tokenizer.trigger_token = '[T]'\n",
    "    tokenizer.trigger_token_id = tokenizer.convert_tokens_to_ids('[T]')\n",
    "    tokenizer.predict_token = '[P]'\n",
    "    tokenizer.predict_token_id = tokenizer.convert_tokens_to_ids('[P]')\n",
    "    # NOTE: BERT and RoBERTa tokenizers work properly if [X] is not a special token...\n",
    "    # tokenizer.lama_x = '[X]'\n",
    "    # tokenizer.lama_x_id = tokenizer.convert_tokens_to_ids('[X]')\n",
    "    tokenizer.lama_y = '[Y]'\n",
    "    tokenizer.lama_x_id = tokenizer.convert_tokens_to_ids('[Y]')\n",
    "\n",
    "def load_pretrained(model_name):\n",
    "    \"\"\"\n",
    "    Loads pretrained HuggingFace config/model/tokenizer, as well as performs required\n",
    "    initialization steps to facilitate working with triggers.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    add_task_specific_tokens(tokenizer)\n",
    "    return config, model, tokenizer\n",
    "\n",
    "logger.info('Loading model, tokenizer, etc.')\n",
    "config, model, tokenizer = load_pretrained(args.model_name)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, config):\n",
    "    \"\"\"Returns the wordpiece embedding module.\"\"\"\n",
    "    base_model = getattr(model, config.model_type)\n",
    "    embeddings = base_model.embeddings.word_embeddings\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientStorage:\n",
    "    \"\"\"\n",
    "    This object stores the intermediate gradients of the output a the given PyTorch module, which\n",
    "    otherwise might not be retained.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        self._stored_gradient = None\n",
    "        module.register_backward_hook(self.hook)\n",
    "\n",
    "    def hook(self, module, grad_in, grad_out):\n",
    "        self._stored_gradient = grad_out[0]\n",
    "\n",
    "    def get(self):\n",
    "        return self._stored_gradient\n",
    "    \n",
    "embedding_gradient = GradientStorage(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask):\n",
    "    \"\"\"Replaces the trigger tokens in input_ids.\"\"\"\n",
    "    out = model_inputs.copy()\n",
    "    input_ids = model_inputs['input_ids']\n",
    "    trigger_ids = trigger_ids.repeat(trigger_mask.size(0), 1)\n",
    "    try:\n",
    "        filled = input_ids.masked_scatter(trigger_mask, trigger_ids)\n",
    "    except RuntimeError:\n",
    "        filled = input_ids\n",
    "    out['input_ids'] = filled\n",
    "    return out\n",
    "\n",
    "class PredictWrapper:\n",
    "    \"\"\"\n",
    "    PyTorch transformers model wrapper. Handles necc. preprocessing of inputs for triggers\n",
    "    experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, model_inputs, trigger_ids):\n",
    "        # Copy dict so pop operations don't have unwanted side-effects\n",
    "        model_inputs = model_inputs.copy()\n",
    "        trigger_mask = model_inputs.pop('trigger_mask')\n",
    "        predict_mask = model_inputs.pop('predict_mask')\n",
    "        model_inputs = replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask)\n",
    "        logits, *_ = self._model(**model_inputs)\n",
    "        predict_logits = logits.masked_select(predict_mask.unsqueeze(-1)).view(logits.size(0), -1)\n",
    "        return predict_logits\n",
    "    \n",
    "predictor = PredictWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.label_map is not None:\n",
    "    label_map = json.loads(args.label_map)\n",
    "    logger.info(f\"Label map: {label_map}\")\n",
    "else:\n",
    "    label_map = None\n",
    "    logger.info('No label map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(tokenizer, label, tokenize=False):\n",
    "    \"\"\"\n",
    "    Helper function for encoding labels. Deals with the subtleties of handling multiple tokens.\n",
    "    \"\"\"\n",
    "    if isinstance(label, str):\n",
    "        if tokenize:\n",
    "            # Ensure label is properly tokenized, and only retain first token\n",
    "            # if it gets split into multiple tokens. TODO: Make sure this is\n",
    "            # desired behavior.\n",
    "            tokens = tokenizer.tokenize(label)\n",
    "            if len(tokens) > 1:\n",
    "                raise ValueError(f'Label \"{label}\" gets mapped to multiple tokens.')\n",
    "            if tokens[0] == tokenizer.unk_token:\n",
    "                raise ValueError(f'Label \"{label}\" gets mapped to unk.')\n",
    "            label = tokens[0]\n",
    "        encoded = torch.tensor(tokenizer.convert_tokens_to_ids([label])).unsqueeze(0)\n",
    "    elif isinstance(label, list):\n",
    "        encoded = torch.tensor(tokenizer.convert_tokens_to_ids(label)).unsqueeze(0)\n",
    "    elif isinstance(label, int):\n",
    "        encoded = torch.tensor([[label]])\n",
    "    return encoded\n",
    "\n",
    "class TriggerTemplatizer:\n",
    "    \"\"\"\n",
    "    An object to facilitate creating transformers-friendly triggers inputs from a template.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    template : str\n",
    "        The template string, comprised of the following tokens:\n",
    "            [T] to mark a trigger placeholder.\n",
    "            [P] to mark a prediction placeholder.\n",
    "            {fields} arbitrary fields instantiated from the dataset instances.\n",
    "        For example a NLI template might look like:\n",
    "            \"[T] [T] [T] {premise} [P] {hypothesis}\"\n",
    "    tokenizer : PretrainedTokenizer\n",
    "        A HuggingFace tokenizer. Must have special trigger and predict tokens.\n",
    "    add_special_tokens : bool\n",
    "        Whether or not to add special tokens when encoding. Default: False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 template,\n",
    "                 config,\n",
    "                 tokenizer,\n",
    "                 label_field='label',\n",
    "                 label_map=None,\n",
    "                 tokenize_labels=False,\n",
    "                 add_special_tokens=False,\n",
    "                 use_ctx=False):\n",
    "        if not hasattr(tokenizer, 'predict_token') or \\\n",
    "           not hasattr(tokenizer, 'trigger_token'):\n",
    "            raise ValueError(\n",
    "                'Tokenizer missing special trigger and predict tokens in vocab.'\n",
    "                'Use `utils.add_special_tokens` to add them.'\n",
    "            )\n",
    "        self._template = template\n",
    "        self._config = config\n",
    "        self._tokenizer = tokenizer\n",
    "        self._label_field = label_field\n",
    "        self._label_map = label_map\n",
    "        self._tokenize_labels = tokenize_labels\n",
    "        self._add_special_tokens = add_special_tokens\n",
    "        self._use_ctx = use_ctx\n",
    "\n",
    "    @property\n",
    "    def num_trigger_tokens(self):\n",
    "        return sum(token == '[T]' for token in self._template.split())\n",
    "\n",
    "    def __call__(self, format_kwargs):\n",
    "        # Format the template string\n",
    "        format_kwargs = format_kwargs.copy()\n",
    "        label = format_kwargs.pop(self._label_field)\n",
    "        text = self._template.format(**format_kwargs)\n",
    "        if label is None:\n",
    "            raise Exception(f'Bad data: {text}')\n",
    "\n",
    "        # Have the tokenizer encode the text and process the output to:\n",
    "        # - Create a trigger and predict mask\n",
    "        # - Replace the predict token with a mask token\n",
    "        model_inputs = self._tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=self._add_special_tokens,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = model_inputs['input_ids']\n",
    "        trigger_mask = input_ids.eq(self._tokenizer.trigger_token_id)\n",
    "        predict_mask = input_ids.eq(self._tokenizer.predict_token_id)\n",
    "        input_ids[predict_mask] = self._tokenizer.mask_token_id\n",
    "\n",
    "        model_inputs['trigger_mask'] = trigger_mask\n",
    "        model_inputs['predict_mask'] = predict_mask\n",
    "\n",
    "        # For relation extraction with BERT, update token_type_ids to reflect the two different sequences\n",
    "        if self._use_ctx and self._config.model_type == 'bert':\n",
    "            sep_token_indices = (input_ids.squeeze(0) == self._tokenizer.convert_tokens_to_ids(self._tokenizer.sep_token)).nonzero().flatten()\n",
    "            sequence_b_indices = torch.arange(sep_token_indices[0], sep_token_indices[1] + 1).long().unsqueeze(0)\n",
    "            model_inputs['token_type_ids'].scatter_(1, sequence_b_indices, 1)\n",
    "\n",
    "        # Encode the label(s)\n",
    "        if self._label_map is not None:\n",
    "            label = self._label_map[label]\n",
    "        label_id = encode_label(\n",
    "            tokenizer=self._tokenizer,\n",
    "            label=label,\n",
    "            tokenize=self._tokenize_labels\n",
    "        )\n",
    "\n",
    "        return model_inputs, label_id\n",
    "\n",
    "templatizer = TriggerTemplatizer(\n",
    "    args.template,\n",
    "    config,\n",
    "    tokenizer,\n",
    "    label_map=label_map,\n",
    "    label_field=args.label_field,\n",
    "    tokenize_labels=args.tokenize_labels,\n",
    "    add_special_tokens=False,\n",
    "    use_ctx=args.use_ctx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the initial trigger tokens and label mapping\n",
    "if args.initial_trigger:\n",
    "    trigger_ids = tokenizer.convert_tokens_to_ids(args.initial_trigger)\n",
    "    logger.debug(f'Initial trigger: {args.initial_trigger}')\n",
    "    logger.debug(f'Trigger ids: {trigger_ids}')\n",
    "    assert len(trigger_ids) == templatizer.num_trigger_tokens\n",
    "else:\n",
    "    trigger_ids = [tokenizer.mask_token_id] * templatizer.num_trigger_tokens\n",
    "trigger_ids = torch.tensor(trigger_ids, device=device).unsqueeze(0)\n",
    "best_trigger_ids = trigger_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Accuracy can only be computed if a fixed pool of labels is given, which currently\n",
    "# requires the label map to be specified. Since producing a label map may be cumbersome (e.g.,\n",
    "# for link prediction tasks), we just use (negative) loss as the evaluation metric in these cases.\n",
    "class AccuracyFn:\n",
    "    \"\"\"\n",
    "    Computing the accuracy when a label is mapped to multiple tokens is difficult in the current\n",
    "    framework, since the data generator only gives us the token ids. To get around this we\n",
    "    compare the target logp to the logp of all labels. If target logp is greater than all (but)\n",
    "    one of the label logps we know we are accurate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, label_map, device, tokenize_labels=False):\n",
    "        self._all_label_ids = []\n",
    "        self._pred_to_label = []\n",
    "        logger.info(label_map)\n",
    "        for label, label_tokens in label_map.items():\n",
    "            self._all_label_ids.append(encode_label(tokenizer, label_tokens, tokenize_labels).to(device))\n",
    "            self._pred_to_label.append(label)\n",
    "        logger.info(self._all_label_ids)\n",
    "\n",
    "    def __call__(self, predict_logits, gold_label_ids):\n",
    "        # Get total log-probability for the true label\n",
    "        gold_logp = get_loss(predict_logits, gold_label_ids)\n",
    "\n",
    "        # Get total log-probability for all labels\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "\n",
    "        # Add up the number of entries where loss is greater than or equal to gold_logp.\n",
    "        ge_count = all_label_logp.le(gold_logp.unsqueeze(-1)).sum(-1)\n",
    "        correct = ge_count.le(1)  # less than in case of num. prec. issues\n",
    "\n",
    "        return correct.float()\n",
    "\n",
    "    # TODO: @rloganiv - This is hacky. Replace with something sensible.\n",
    "    def predict(self, predict_logits):\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "def get_loss(predict_logits, label_ids):\n",
    "    predict_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    target_logp = predict_logp.gather(-1, label_ids)\n",
    "    target_logp = target_logp - 1e32 * label_ids.eq(0)  # Apply mask\n",
    "    target_logp = torch.logsumexp(target_logp, dim=-1)\n",
    "    return -target_logp\n",
    "\n",
    "if label_map:\n",
    "    evaluation_fn = AccuracyFn(tokenizer, label_map, device)\n",
    "else:\n",
    "    evaluation_fn = lambda x, y: -get_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_squeeze_sequence(sequence, *args, **kwargs):\n",
    "    \"\"\"Squeezes fake batch dimension added by tokenizer before padding sequence.\"\"\"\n",
    "    return pad_sequence([x.squeeze(0) for x in sequence], *args, **kwargs)\n",
    "\n",
    "class Collator:\n",
    "    \"\"\"\n",
    "    Collates transformer outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id=0):\n",
    "        self._pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Separate the list of inputs and labels\n",
    "        model_inputs, labels = list(zip(*features))\n",
    "        # Assume that all inputs have the same keys as the first\n",
    "        proto_input = model_inputs[0]\n",
    "        keys = list(proto_input.keys())\n",
    "        padded_inputs = {}\n",
    "        for key in keys:\n",
    "            if key == 'input_ids':\n",
    "                padding_value = self._pad_token_id\n",
    "            else:\n",
    "                padding_value = 0\n",
    "            # NOTE: We need to squeeze to get rid of fake batch dim.\n",
    "            sequence = [x[key] for x in model_inputs]\n",
    "            padded = pad_squeeze_sequence(sequence, batch_first=True, padding_value=padding_value)\n",
    "            padded_inputs[key] = padded\n",
    "        labels = pad_squeeze_sequence(labels, batch_first=True, padding_value=0)\n",
    "        return padded_inputs, labels\n",
    "\n",
    "logger.info('Loading datasets')\n",
    "collator = Collator(pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_LEN = 50\n",
    "\n",
    "def load_tsv(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "\n",
    "def load_jsonl(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "LOADERS = {\n",
    "    '.tsv': load_tsv,\n",
    "    '.jsonl': load_jsonl\n",
    "}\n",
    "\n",
    "\n",
    "def load_trigger_dataset(fname, templatizer, use_ctx, limit=None):\n",
    "    loader = LOADERS[fname.suffix]\n",
    "    instances = []\n",
    "\n",
    "    for x in loader(fname):\n",
    "        try:\n",
    "            if use_ctx:\n",
    "                # For relation extraction, skip facts that don't have context sentence\n",
    "                if 'evidences' not in x:\n",
    "                    logger.warning('Skipping RE sample because it lacks context sentences: {}'.format(x))\n",
    "                    continue\n",
    "\n",
    "                evidences = x['evidences']\n",
    "                    \n",
    "                # Randomly pick a context sentence\n",
    "                obj_surface, masked_sent = random.choice([(evidence['obj_surface'], evidence['masked_sentence']) for evidence in evidences])\n",
    "                words = masked_sent.split()\n",
    "                if len(words) > MAX_CONTEXT_LEN:\n",
    "                    # If the masked sentence is too long, use the first X tokens. For training we want to keep as many samples as we can.\n",
    "                    masked_sent = ' '.join(words[:MAX_CONTEXT_LEN])\n",
    "                \n",
    "                # If truncated context sentence still has MASK, we need to replace it with object surface\n",
    "                # We explicitly use [MASK] because all TREx fact's context sentences use it\n",
    "                context = masked_sent.replace('[MASK]', obj_surface)\n",
    "                x['context'] = context\n",
    "                model_inputs, label_id = templatizer(x)\n",
    "            else:\n",
    "                model_inputs, label_id = templatizer(x)\n",
    "        except ValueError as e:\n",
    "            logger.warning('Encountered error \"%s\" when processing \"%s\".  Skipping.', e, x)\n",
    "            continue\n",
    "        else:\n",
    "            instances.append((model_inputs, label_id))\n",
    "    if limit:\n",
    "        return random.sample(instances, limit)\n",
    "    else:\n",
    "        return instances\n",
    "\n",
    "# if args.perturbed:\n",
    "#     train_dataset = load_augmented_trigger_dataset(args.train, templatizer, limit=args.limit)\n",
    "# else:\n",
    "train_dataset = load_trigger_dataset(args.train, templatizer, use_ctx=args.use_ctx, limit=args.limit)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.bsz, shuffle=True, collate_fn=collator)\n",
    "\n",
    "# if args.perturbed:\n",
    "#     dev_dataset = utils.load_augmented_trigger_dataset(args.dev, templatizer)\n",
    "# else:\n",
    "dev_dataset = load_trigger_dataset(args.dev, templatizer, use_ctx=args.use_ctx)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=args.eval_size, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To \"filter\" unwanted trigger tokens, we subtract a huge number from their logits.\n",
    "def isupper(idx, tokenizer):\n",
    "    \"\"\"\n",
    "    Determines whether a token (e.g., word piece) begins with a capital letter.\n",
    "    \"\"\"\n",
    "    _isupper = False\n",
    "    # We only want to check tokens that begin words. Since byte-pair encoding\n",
    "    # captures a prefix space, we need to check that the decoded token begins\n",
    "    # with a space, and has a capitalized second character.\n",
    "    if isinstance(tokenizer, transformers.GPT2Tokenizer):\n",
    "        decoded = tokenizer.decode([idx])\n",
    "        if decoded[0] == ' ' and decoded[1].isupper():\n",
    "            _isupper = True\n",
    "    # For all other tokenization schemes, we can just check the first character\n",
    "    # is capitalized.\n",
    "    elif tokenizer.decode([idx])[0].isupper():\n",
    "            _isupper = True\n",
    "    return _isupper\n",
    "\n",
    "\n",
    "filter = torch.zeros(tokenizer.vocab_size, dtype=torch.float32, device=device)\n",
    "if args.filter:\n",
    "    logger.info('Filtering label tokens.')\n",
    "    if label_map:\n",
    "        for label_tokens in label_map.values():\n",
    "            label_ids = encode_label(tokenizer, label_tokens).unsqueeze(0)\n",
    "            filter[label_ids] = -1e32\n",
    "    else:\n",
    "        for _, label_ids in train_dataset:\n",
    "            filter[label_ids] = -1e32\n",
    "    logger.info('Filtering special tokens and capitalized words.')\n",
    "    for word, idx in tokenizer.get_vocab().items():\n",
    "        if len(word) == 1 or idx >= tokenizer.vocab_size:\n",
    "            continue\n",
    "        # Filter special tokens.\n",
    "        if idx in tokenizer.all_special_ids:\n",
    "            logger.debug('Filtered: %s', word)\n",
    "            filter[idx] = -1e32\n",
    "        # Filter capitalized words (lazy way to remove proper nouns).\n",
    "        if isupper(idx, tokenizer):\n",
    "            logger.debug('Filtered: %s', word)\n",
    "            filter[idx] = -1e32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Evaluating')\n",
    "numerator = 0\n",
    "denominator = 0\n",
    "for model_inputs, labels in tqdm(dev_loader):\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        predict_logits = predictor(model_inputs, trigger_ids)\n",
    "    numerator += evaluation_fn(predict_logits, labels).sum().item()\n",
    "    denominator += labels.size(0)\n",
    "dev_metric = numerator / (denominator + 1e-13)\n",
    "logger.info(f'Dev metric: {dev_metric}')\n",
    "\n",
    "best_dev_metric = -float('inf')\n",
    "# Measure elapsed time of trigger search\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotflip_attack(averaged_grad,\n",
    "                   embedding_matrix,\n",
    "                   increase_loss=False,\n",
    "                   num_candidates=1,\n",
    "                   filter=None):\n",
    "    \"\"\"Returns the top candidate replacements.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        gradient_dot_embedding_matrix = torch.matmul(\n",
    "            embedding_matrix,\n",
    "            averaged_grad\n",
    "        )\n",
    "        if filter is not None:\n",
    "            gradient_dot_embedding_matrix -= filter\n",
    "        if not increase_loss:\n",
    "            gradient_dot_embedding_matrix *= -1\n",
    "        _, top_k_ids = gradient_dot_embedding_matrix.topk(num_candidates)\n",
    "\n",
    "    return top_k_ids\n",
    "\n",
    "for i in range(args.iters):\n",
    "\n",
    "    logger.info(f'Iteration: {i}')\n",
    "\n",
    "    logger.info('Accumulating Gradient')\n",
    "    model.zero_grad()\n",
    "\n",
    "    pbar = tqdm(range(args.accumulation_steps))\n",
    "    train_iter = iter(train_loader)\n",
    "    averaged_grad = None\n",
    "\n",
    "    # Accumulate\n",
    "    for step in pbar:\n",
    "\n",
    "        # Shuttle inputs to GPU\n",
    "        try:\n",
    "            model_inputs, labels = next(train_iter)\n",
    "        except:\n",
    "            logger.warning(\n",
    "                'Insufficient data for number of accumulation steps. '\n",
    "                'Effective batch size will be smaller than specified.'\n",
    "            )\n",
    "            break\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        predict_logits = predictor(model_inputs, trigger_ids)\n",
    "        loss = get_loss(predict_logits, labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        grad = embedding_gradient.get()\n",
    "        bsz, _, emb_dim = grad.size()\n",
    "        selection_mask = model_inputs['trigger_mask'].unsqueeze(-1)\n",
    "        grad = torch.masked_select(grad, selection_mask)\n",
    "        grad = grad.view(bsz, templatizer.num_trigger_tokens, emb_dim)\n",
    "\n",
    "        if averaged_grad is None:\n",
    "            averaged_grad = grad.sum(dim=0) / args.accumulation_steps\n",
    "        else:\n",
    "            averaged_grad += grad.sum(dim=0) / args.accumulation_steps\n",
    "\n",
    "    logger.info('Evaluating Candidates')\n",
    "    pbar = tqdm(range(args.accumulation_steps))\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    token_to_flip = random.randrange(templatizer.num_trigger_tokens)\n",
    "    candidates = hotflip_attack(averaged_grad[token_to_flip],\n",
    "                                embeddings.weight,\n",
    "                                increase_loss=False,\n",
    "                                num_candidates=args.num_cand,\n",
    "                                filter=filter)\n",
    "\n",
    "    current_score = 0\n",
    "    candidate_scores = torch.zeros(args.num_cand, device=device)\n",
    "    denom = 0\n",
    "    for step in pbar:\n",
    "\n",
    "        try:\n",
    "            model_inputs, labels = next(train_iter)\n",
    "        except:\n",
    "            logger.warning(\n",
    "                'Insufficient data for number of accumulation steps. '\n",
    "                'Effective batch size will be smaller than specified.'\n",
    "            )\n",
    "            break\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            predict_logits = predictor(model_inputs, trigger_ids)\n",
    "            eval_metric = evaluation_fn(predict_logits, labels)\n",
    "\n",
    "        # Update current score\n",
    "        current_score += eval_metric.sum()\n",
    "        denom += labels.size(0)\n",
    "\n",
    "        # NOTE: Instead of iterating over tokens to flip we randomly change just one each\n",
    "        # time so the gradients don't get stale.\n",
    "        for i, candidate in enumerate(candidates):\n",
    "\n",
    "            # if candidate.item() in filter_candidates:\n",
    "            #     candidate_scores[i] = -1e32\n",
    "            #     continue\n",
    "\n",
    "            temp_trigger = trigger_ids.clone()\n",
    "            temp_trigger[:, token_to_flip] = candidate\n",
    "            with torch.no_grad():\n",
    "                predict_logits = predictor(model_inputs, temp_trigger)\n",
    "                eval_metric = evaluation_fn(predict_logits, labels)\n",
    "\n",
    "            candidate_scores[i] += eval_metric.sum()\n",
    "\n",
    "    # TODO: Something cleaner. LAMA templates can't have mask tokens, so if\n",
    "    # there are still mask tokens in the trigger then set the current score\n",
    "    # to -inf.\n",
    "    if args.print_lama:\n",
    "        if trigger_ids.eq(tokenizer.mask_token_id).any():\n",
    "            current_score = float('-inf')\n",
    "\n",
    "    if (candidate_scores > current_score).any():\n",
    "        logger.info('Better trigger detected.')\n",
    "        best_candidate_score = candidate_scores.max()\n",
    "        best_candidate_idx = candidate_scores.argmax()\n",
    "        trigger_ids[:, token_to_flip] = candidates[best_candidate_idx]\n",
    "        logger.info(f'Train metric: {best_candidate_score / (denom + 1e-13): 0.4f}')\n",
    "    else:\n",
    "        logger.info('No improvement detected. Skipping evaluation.')\n",
    "        continue\n",
    "\n",
    "    logger.info('Evaluating')\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for model_inputs, labels in tqdm(dev_loader):\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            predict_logits = predictor(model_inputs, trigger_ids)\n",
    "        numerator += evaluation_fn(predict_logits, labels).sum().item()\n",
    "        denominator += labels.size(0)\n",
    "    dev_metric = numerator / (denominator + 1e-13)\n",
    "\n",
    "    logger.info(f'Trigger tokens: {tokenizer.convert_ids_to_tokens(trigger_ids.squeeze(0))}')\n",
    "    logger.info(f'Dev metric: {dev_metric}')\n",
    "\n",
    "    # TODO: Something cleaner. LAMA templates can't have mask tokens, so if\n",
    "    # there are still mask tokens in the trigger then set the current score\n",
    "    # to -inf.\n",
    "    if args.print_lama:\n",
    "        if best_trigger_ids.eq(tokenizer.mask_token_id).any():\n",
    "            best_dev_metric = float('-inf')\n",
    "\n",
    "    if dev_metric > best_dev_metric:\n",
    "        logger.info('Best performance so far')\n",
    "        best_trigger_ids = trigger_ids.clone()\n",
    "        best_dev_metric = dev_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trigger_tokens = tokenizer.convert_ids_to_tokens(best_trigger_ids.squeeze(0))\n",
    "logger.info(f'Best tokens: {best_trigger_tokens}')\n",
    "logger.info(f'Best dev metric: {best_dev_metric}')\n",
    "if args.print_lama:\n",
    "    # Templatize with [X] and [Y]\n",
    "    if args.use_ctx:\n",
    "        model_inputs, label_ids = templatizer({\n",
    "            'sub_label': '[X]',\n",
    "            'obj_label': tokenizer.lama_y,\n",
    "            'context': ''\n",
    "        })\n",
    "    else:\n",
    "        model_inputs, label_ids = templatizer({\n",
    "            'sub_label': '[X]',\n",
    "            'obj_label': tokenizer.lama_y,\n",
    "        })\n",
    "    lama_template = model_inputs['input_ids']\n",
    "    # Instantiate trigger tokens\n",
    "    lama_template.masked_scatter_(\n",
    "        mask=model_inputs['trigger_mask'],\n",
    "        source=best_trigger_ids.cpu())\n",
    "    # Instantiate label token\n",
    "    lama_template.masked_scatter_(\n",
    "        mask=model_inputs['predict_mask'],\n",
    "        source=label_ids)\n",
    "    # Print LAMA JSON template\n",
    "    relation = args.train.parent.stem\n",
    "\n",
    "    # The following block of code is a bit hacky but whatever, it gets the job done\n",
    "    if args.use_ctx:\n",
    "        template = tokenizer.decode(lama_template.squeeze(0)[1:-1]).replace('[SEP] ', '').replace('</s> ', '').replace('[ X ]', '[X]')\n",
    "    else:\n",
    "        template = tokenizer.decode(lama_template.squeeze(0)[1:-1]).replace('[ X ]', '[X]')\n",
    "\n",
    "    out = {\n",
    "        'relation': args.train.parent.stem,\n",
    "        'template': template\n",
    "    }\n",
    "    print(json.dumps(out))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef5ebbdb7177042242d5c612e2b6c40616b7b35a1a7eb91b6720a1b19cc3ee4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('autoprompt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
